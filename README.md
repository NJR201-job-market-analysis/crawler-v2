# 分散式職缺爬蟲專案

## 專案簡介

本專案為一個模組化、可擴展的分散式爬蟲系統，旨在從多個主流招聘平台高效地收集並整合職缺資訊，以進行後續的就業市場數據分析與洞察。

目前支援的平台包含：
*   104 人力銀行
*   Yourator
*   CakeResume
*   1111 人力銀行
*   LinkedIn

## 核心架構

本專案的核心基於 **生產者-消費者 (Producer-Consumer)** 架構，透過一個中央的 **訊息佇列 (Message Queue)** 來解耦任務的產生與執行，從而實現高併發與分散式處理。

*   **生產者 (Producers)**：針對各個招聘平台，獨立的生產者程序會持續掃描職缺列表頁，一旦發現新的或更新的職缺，便會將該職缺的 URL 作為一個任務，發布到訊息佇列中。
*   **訊息佇列 (Message Queue)**：作為中介層，負責接收並暫存所有來自生產者的任務。它是整個系統的任務緩衝區，確保任務不會遺失，並等待消費者處理。
*   **消費者/工作節點 (Consumers/Workers)**：可以部署在多台機器上的工作節點，會持續監聽訊息佇列。一旦有新的任務，工作節點就會取回任務（即職缺 URL），並執行對應平台的爬蟲邏輯來抓取詳細資料，最後將結果存入資料庫。

## 程式執行流程

1.  **啟動生產者**：針對特定平台（如 104）執行其 `producer.py`。
2.  **產生任務**：生產者開始瀏覽該平台的職缺列表，將每個職缺的獨立頁面 URL 封裝成一個任務，並發送到訊息佇列。
3.  **啟動消費者**：執行通用的 `worker.py`，啟動一個或多個工作節點。
4.  **消費任務**：工作節點從訊息佇列中獲取一個待處理的任務。
5.  **執行爬取**：工作節點根據任務中的資訊，調用相應平台（如 104）的 `crawler.py` 中的核心爬取函式。
6.  **解析與儲存**：`crawler.py` 負責請求職缺 URL、解析 HTML 並提取所需欄位（如公司名稱、薪資、技能要求等）。
7.  **寫入資料庫**：爬取到的結構化資料，透過 `shared/db.py` 的輔助函式，被寫入到後端資料庫中，完成一次完整的流程。

## 專案結構

### `crawlers/`

此目錄包含了所有平台爬蟲的主要邏輯，每個子目錄對應一個平台。

在每個平台目錄下（例如 `crawlers/104/`），通常包含以下檔案：

*   `main.py`: 執行該平台爬蟲的腳本進入點，可用於手動觸發或調度。
*   `producer.py`: **生產者**，負責掃描職缺列表頁，並將待爬取的職缺 URL 任務發布到訊息佇列。
*   `crawler.py`: **核心爬蟲**，包含解析單一職缺頁面、提取詳細資訊的具體邏輯。
*   `tasks.py`: 定義背景任務（例如使用 Celery），它會包裹 `crawler.py` 中的函式，使其能被 Worker 非同步調用。
*   `constants.py`: 存放與該平台相關的常數，例如 API 端點、查詢參數、URL 模板等。

### `shared/`

此目錄提供所有爬蟲模組共用的功能，以避免重複撰寫程式碼。

*   `config.py`: 負責讀取 `local.ini` 等設定檔，提供統一的設定存取接口，供各模組獲取資料庫連線資訊、API 金鑰等。
*   `db.py`: 封裝資料庫連線與操作，例如提供連線池、標準化的資料寫入/更新方法等，簡化資料庫互動。
*   `logger.py`: 設定全域的日誌紀錄器 (Logger)，統一日誌格式與輸出位置，方便追蹤與除錯。
*   `files.py`: 提供通用的檔案讀寫輔助函式。
*   `__mock__.py`: 用於單元測試，提供模擬 (Mock) 的資料或物件，以便在沒有真實外部依賴（如資料庫）的情況下進行測試。

## 詳細資訊

關於環境設定、執行與部署的詳細操作流程，請參考文件：`docs/crawler.md`。

from shared.logger import logger
from .tasks import crawl_1111_jobs
from .constants import JOB_CATEGORIES

# å®šç¾©è¦çˆ¬å–çš„åˆ†ã„Š
categories = [
    # é›²æœå‹™/é›²è¨ˆç®—
    "140600",
    # ç¶²ç«™æŠ€è¡“ä¸»ç®¡
    "140211",
    # ç¶²ç«™ç¨‹å¼è¨­è¨ˆå¸«
    "140213",
    "140802",
    "140803",
    "140804",
    "140805",
    "140806",
    "140810",
    "140301",
    # å¯ä»¥ç¹¼çºŒæ·»åŠ æ›´å¤šåˆ†é¡
]

logger.info("ğŸš€ é–‹å§‹ç™¼é€ %s å€‹ 1111 çˆ¬èŸ²ä»»å‹™", len(categories))

tasks = []

# ç‚ºæ¯å€‹åˆ†é¡å‰µå»ºä¸€å€‹ä»»å‹™
for category_id in categories:

    category_name = JOB_CATEGORIES[category_id]

    task = crawl_1111_jobs.s(category_id=category_id)
    task.apply_async(queue="crawler-queue")

    tasks.append(task)
    logger.info(
        "ğŸ“¤ å·²ç™¼é€ 1111 çˆ¬èŸ²ä»»å‹™: %s | %s | ID: %s", category_id, category_name, task.id
    )

logger.info("âœ… æ‰€æœ‰ 1111 çˆ¬èŸ²ä»»å‹™å·²ç™¼é€å®Œæˆï¼Œå…± %s å€‹ä»»å‹™", len(tasks))

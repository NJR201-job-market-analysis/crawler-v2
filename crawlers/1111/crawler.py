import urllib.request as req
import bs4 as bs
import urllib
import ssl
import json
from shared.logger import logger

# from shared.files import save_to_csv
from ..constants import COMMON_SKILLS

# ç›´æ¥çˆ¬æœƒè·³å‡ºsslèªè­‰æ²’é, sslé€™è¡Œæ˜¯googleä¾†çš„æŒ‡ä»¤ è²¼ä¸Šå»å¾Œæ‰å¯ä»¥çˆ¬å–
ssl._create_default_https_context = ssl._create_unverified_context


def analaze_pages(url):
    r = req.Request(
        url,
        headers={
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
        },
    )
    resp = req.urlopen(r)
    content = resp.read().decode("utf-8")
    html = bs.BeautifulSoup(content, "html.parser")

    job_description = html.find("div", {"class": "whitespace-pre-line"}).text
    # skills = html.find_all("p", {"class": "underline-offset-1"})
    # é›»è…¦å°ˆé•·(å‰é¢æ˜¯é›»è…¦å°ˆé•·list, listæœ€å¾Œä¸€é …å»æ˜¯æŠ“å‡ºå…¬å¸åç¨±, æ‰€ä»¥popæ‰.
    # ä½†å¦‚æœå»£å‘Šæ²’æœ‰åˆŠç™»é›»è…¦å°ˆé•·, ä¸€æ¨£æœƒæŠ“åˆ°å…¬å¸åç¨±, æ‰€ä»¥æ²¿ç”¨popå¾Œè®Šæˆçš„ç©ºé›†åˆ, è³¦å€¼"æ²’æœ‰åˆŠç™»é›»è…¦å°ˆé•·"
    # skill_list = []
    # for skill in skills:
    #     skill_list.append(skill.text)
    # skill_list.pop()
    # if skill_list == []:
    #     skill_list = "æ²’æœ‰åˆŠç™»é›»è…¦å°ˆé•·"

    # åœ°å€çš„æ¨™é ­å’Œ"class"æœ‰å…©ç¨®...#çˆ¬å‡ºä¾†çš„åœ°å€æœƒåƒé›œä¸€å †ç©ºæ ¼å’Œæ›è¡Œ, é€™è£¡å…ˆè¨­ä¸€å€‹ç©ºlist,
    # å†ç”¨for inå›åœˆå¯«å…¥æœ‰æ„ç¾©çš„æ–‡å­—(ç©ºæ ¼å’Œæ›è¡Œç¬¦è™Ÿä¸å¯«å…¥)
    work_place = html.find("p", {"class": "mb-4"})  # ç¬¬ä¸€ç¨®è·¯å¾‘
    if work_place == None:  # å¦‚æœç¬¬ä¸€ç¨®è·¯å¾‘æŠ“ä¸åˆ°
        work_place = html.find(
            "div", {"class": "md:items-center content"}
        ).p.text  # å°±å®ƒäº†
        work_place_location = ""
        for w in work_place:
            if w == " " or w == "\n":
                continue
        else:
            work_place_location = work_place_location + w
    else:
        work_place_location = ""
        for s in work_place.text:
            if s == " " or s == "\n":
                continue  # ä¸å¯«å…¥ç©ºæ ¼" "å’Œæ›è¡Œç¬¦è™Ÿ"\n" é‡åˆ°å°±å›å‰é¢, ä¸åŸ·è¡Œappend(a)
            work_place_location = work_place_location + s

    # é ç«¯ æœ‰çš„å…¬å¸ç›´æ¥ç¼ºå°‘è©²æ¬„ä½ ç”¨classæœƒæ’ˆåˆ°åˆ¥çš„æ±è¥¿ æ‰€ä»¥æŸ¥æ‰¾é—œéµå­—"é ç«¯ä¸Šç­"çš„"é "
    # (ä¸ç”¨"ç«¯"æ˜¯å› ç‚ºæœƒæ’ˆå‡ºå‰ç«¯å¾Œç«¯)(æ‡‰è©²æœ‰æ›´å¥½çš„è§£æ³•, å¾…æˆ‘å†ç ”ç©¶XD)
    contents = html.find_all("div", {"class": "content"})

    keywords = {
        "é ",
    }
    work_type = set()
    for w in contents:
        if w.h3 != None:
            for keyword in w.h3.text:
                if keyword in keywords:
                    work_type.add(w.h3.text)
        if w.p != None:
            for keyword in w.p.text:
                if keyword in keywords:
                    work_type.add(w.p.text)
    if work_type == set():  # QQå…±ç”¨çš„å¤ªå¤šäº†, ä¸€ç›´é‡è¤‡æŸ¥æ‰¾, å…ˆç”¨set()åˆªæ‰é‡è¤‡æŠ“åˆ°çš„
        work_type = "æ²’æœ‰é è·"
    else:
        work_type = ",".join(work_type)
        if work_type == "ç™¼å±•é æ™¯":
            work_type = "æ²’æœ‰é è·"

    job_experience = ""
    job_category = ""
    job_salary = ""
    job_skills = set()
    # å…ˆå¾è·ç¼ºæè¿°ä¸­æå–æŠ€èƒ½
    job_skills.update(_extract_job_skills_from_job_description(job_description))

    for content in contents:
        if content.h3 != None:
            if content.h3.text == "å·¥ä½œç¶“é©—":
                # print("å·¥ä½œç¶“é©—", content.p.text)
                job_experience = content.p.text
            if content.h3.text == "è·å‹™é¡åˆ¥":
                job_category = ";".join(content.p.text.split("ã€"))
            if content.h3.text == "å·¥ä½œå¾…é‡":
                job_salary = (
                    next(content.select_one("div.text-main").stripped_strings, "")
                    .replace("(ç¶“å¸¸æ€§è–ªè³‡é”4è¬å…ƒæˆ–ä»¥ä¸Š)", "")
                    .replace("(å›ºå®šæˆ–è®Šå‹•è–ªè³‡å› å€‹äººè³‡æ­·æˆ–ç¸¾æ•ˆè€Œç•°)", "")
                )
            if content.h3.text == "é›»è…¦å°ˆé•·":
                job_skills.update(_extract_job_skills_from_computer_field(content))
            if content.h3.text == "é™„åŠ æ¢ä»¶":
                job_skills.update(_extract_job_skills_from_additional_field(content))

    # å‡ºå·®
    # for job_trip in contents:
    #     if job_trip.h3 != None:
    #         if job_trip.h3 == "å…¶ä»–èªªæ˜":
    #             job_trips = job_trip.p.text
    #         else:
    #             job_trips = "ä¸ç”¨å‡ºå·®"

    # æ›´æ–°æ™‚é–“
    updata_time = html.find("span", {"class": "leading-[1.8] text-[16px]"}).text

    data = {
        "description": job_description,
        "skills": ",".join(sorted(job_skills)) if job_skills else "",
        "experience": job_experience,
        "work_type": work_type,
        # "å…¶ä»–èªªæ˜": job_trips,
        "salary": job_salary,
        "update_time": updata_time,
        "category": job_category,
    }
    return data


# æŸ¥æ‰¾çš„é—œéµå­— ä¸­é–“ç”¨+é€£æ¥ ex.software+engineer
looking_for = urllib.parse.quote("è»Ÿé«”+å·¥ç¨‹å¸«")
looking_for = "desc&ks=" + urllib.parse.quote("è»Ÿé«”+å·¥ç¨‹å¸«")  # ä¸­æ–‡é—œéµå­—
p_start = 1  # å¾ç¬¬å¹¾é é–‹å§‹æ‰¾
p_limit = 2  # æ‰¾åˆ°ç¬¬å¹¾é 
wfh = False  # é è¨­ä¸è¨­æ¢ä»¶. ä½†åªæƒ³æ‰¾é ç«¯å·¥ä½œ, æ”¹æˆTrue
ex = False  # é è¨­ä¸è¨­æ¢ä»¶. ä½†è¦æ‰¾å·¥ä½œç¶“é©—"ä¸æ‹˜"çš„, æ”¹æˆFalse
w_place = []  # é è¨­ä¸é™, è‹¥è¦ç‰¹å®šçš„ç¸£å¸‚, è¼¸å…¥"å…©å€‹å­—", ä¸¦ä»¥é€—è™Ÿç›¸éš” ex.["å°åŒ—","æ¡ƒåœ’"]
table = []  # é€™æ˜¯å¼µç„¡é—œç·Šè¦çš„æ¡Œå­


def find_all_pages(looking_for, p_start, p_limit, w_place, wfh, ex):
    for i in range(p_start, p_limit + 1):
        url = f"https://www.1111.com.tw/search/job?page={i}&col=ab&sort=desc&ks={looking_for}"
        r = req.Request(
            url,
            headers={
                "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
            },
        )
        resp = req.urlopen(r)
        content = resp.read().decode("utf-8")
        html = bs.BeautifulSoup(content, "html.parser")
        job_list = html.find_all("div", {"class": "shrink-0"})
        for h in job_list:
            if h.find("a", {"class": "mb-1"}) != None:
                job_url = (
                    "https://www.1111.com.tw" + h.find("a", {"class": "mb-1"})["href"]
                )
                # print(job_url)
                # print("-"*10)
                data = analaze_pages(job_url)
                if wfh == True and data["é è·å·¥ä½œ"] == "æ²’æœ‰é è·":
                    continue
                if ex == True and data["å·¥ä½œç¶“é©—"] != "ä¸æ‹˜":
                    continue
                if w_place != [] and data["å·¥ä½œåœ°é»"][0:2] not in w_place:
                    continue
                table.append(data)
    return table


# https://www.1111.com.tw/api/v1/search/jobs/?page=1&fromOffset=2&jobPositions=140600"
BASE_URL = "https://www.1111.com.tw/api/v1/search/jobs/"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
}


def crawl_1111_jobs_by_category(category):
    result = []
    page = 1

    category_id = category["id"]
    category_name = category["name"]

    logger.info("ğŸ› é–‹å§‹çˆ¬å– 1111 è·ç¼º | %s | %s", category_id, category_name)

    while True:
        json = fetch_job_list(category_id, page)
        if json is None:
            logger.info("è«‹æ±‚ category:%s, page:%s å¤±æ•—", category_name, page)
            break

        total_page = json["result"]["pagination"]["totalPage"]
        if page == total_page:
            break

        for job in json["result"]["hits"]:
            job_id = job["jobId"]
            job_title = job["title"]
            job_location = job["workCity"]["name"]
            company_name = job["companyName"]
            job_url = f"https://www.1111.com.tw/job/{job_id}"
            job_detail = analaze_pages(job_url)

            data = {
                "job_title": job_title,
                "company_name": company_name,
                "location": job_location,
                "job_url": job_url,
                "job_description": job_detail["description"],
                "required_skills": job_detail["skills"],
                "salary": job_detail["salary"],
                "experience": job_detail["experience"],
                "work_type": job_detail["work_type"],
                "category": "è»Ÿé«” / å·¥ç¨‹é¡äººå“¡",
                "sub_category": category_name,
                "platform": "1111",
            }
            result.append(data)

        page += 1

    # save_to_csv(result, f"1111_jobs_{category_name}")

    return result


def safe_parse_json(res):
    try:
        data = res.read()
        json_str = data.decode("utf-8")
        json_data = json.loads(json_str)
        return json_data
    except Exception as e:
        logger.error("è§£æ JSON å¤±æ•— | %s", e)
        return None


def fetch_job_list(category_id, page):
    try:
        request_url = f"{BASE_URL}?page={page}&jobPositions={category_id}"
        r = req.Request(request_url)
        r.add_header("User-Agent", HEADERS["User-Agent"])
        res = req.urlopen(r)
        return safe_parse_json(res)
    except Exception as e:
        logger.error("è«‹æ±‚ %s å¤±æ•— | %s", request_url, e)
        return None


def _extract_job_skills_from_job_description(description):
    try:
        description = description.lower()
        found_skills = set()
        for skill in COMMON_SKILLS:
            if skill.lower() in description:
                found_skills.add(skill)
        return found_skills
    except Exception as e:
        logger.error("è§£æè·ç¼ºæè¿°å¤±æ•— | %s", e)
        return ""


def _extract_job_skills_from_computer_field(soup):
    try:
        found_skills = set()
        for job_skill in soup.select("li > a > p"):
            for skill in COMMON_SKILLS:
                if skill.lower() in job_skill.text.lower():
                    found_skills.add(skill)
        return found_skills
    except Exception as e:
        logger.error("è§£æé›»è…¦å°ˆé•·å¤±æ•— | %s", e)
        return ""


def _extract_job_skills_from_additional_field(soup):
    try:
        found_skills = set()
        additional_content_text = soup.select_one("div > div > p").text.lower()
        for skill in COMMON_SKILLS:
            if skill.lower() in additional_content_text:
                found_skills.add(skill)
        return found_skills
    except Exception as e:
        logger.error("è§£æé™„åŠ æ¢ä»¶å¤±æ•— | %s", e)
        return ""

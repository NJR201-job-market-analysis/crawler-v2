from datetime import datetime
from shared.logger import logger
from shared.db import Database
from crawlers.worker import app
from .crawler import crawl_1111_jobs_by_category


@app.task(bind=True)
def crawl_1111_jobs(self, category):

    task_id = self.request.id
    start_time = datetime.now()

    try:
        result = crawl_1111_jobs_by_category(category=category)

        logger.info(
            "ğŸ—„ï¸  å¯«å…¥è³‡æ–™åº« | ğŸ“‚ %s | ğŸ“Š %s ç­†",
            category["name"],
            len(result),
        )
        Database().insert_jobs(result)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        logger.info("âœ… 1111 çˆ¬èŸ²ä»»å‹™å®Œæˆ %s | è€—æ™‚: %.2fç§’", task_id, duration)

        return {
            "status": "success",
            "task_id": task_id,
            "category_id": category["id"],
            "category_name": category["name"],
            "duration": duration,
            "result_count": len(result),
            "timestamp": end_time.isoformat(),
        }
    except Exception as e:
        logger.error("âŒ 1111 çˆ¬èŸ²ä»»å‹™å¤±æ•— %s | éŒ¯èª¤: %s", task_id, str(e))

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return {
            "status": "error",
            "task_id": task_id,
            "error": str(e),
            "category_id": category["id"],
            "category_name": category["name"],
            "duration": duration,
            "timestamp": end_time.isoformat(),
        }

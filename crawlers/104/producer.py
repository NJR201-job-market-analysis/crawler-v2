from shared.logger import logger
from .tasks import crawl_104_jobs
from .category_dict import CATEGORIES_DICT

# å®šç¾©è¦çˆ¬å–çš„åˆ†é¡å’Œè·ä½é¡å‹
categories = [
    # è»Ÿé«”å·¥ç¨‹é¡äººå“¡
    "2007001000",
    # å‰ç«¯å·¥ç¨‹å¸«
    "2007001015",
    # å¯ä»¥ç¹¼çºŒæ·»åŠ æ›´å¤šåˆ†é¡
]

logger.info("ğŸš€ é–‹å§‹ç™¼é€ %s å€‹ 104 çˆ¬èŸ²ä»»å‹™", len(categories))

tasks = []

# ç‚ºæ¯å€‹åˆ†é¡å‰µå»ºä¸€å€‹ä»»å‹™
for category, job_type in categories:
    category_name = CATEGORIES_DICT[category]

    task = crawl_104_jobs.s(category_id=category)
    task.apply_async(queue="crawler-queue")

    tasks.append(task)
    logger.info("ğŸ“¤ å·²ç™¼é€ 104 çˆ¬èŸ²ä»»å‹™: %s | ID: %s", category_name, task.id)

logger.info("âœ… æ‰€æœ‰ 104 çˆ¬èŸ²ä»»å‹™å·²ç™¼é€å®Œæˆï¼Œå…± %s å€‹ä»»å‹™", len(tasks))

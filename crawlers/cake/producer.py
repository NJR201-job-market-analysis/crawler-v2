from .tasks import crawl_cake_jobs
from shared.logger import logger

# å®šç¾©è¦çˆ¬å–çš„åˆ†é¡å’Œè·ä½é¡å‹
categories = [
    ("software-developer", None),
    ("start-up", None),
    ("overseas-company", "it"),
    ("million-salary", "it"),
    ("top500-companies", "it"),
    ("digital-nomad", "it"),
    ("remote-work", "it"),
    ("bank", "it"),
    # å¯ä»¥ç¹¼çºŒæ·»åŠ æ›´å¤šåˆ†é¡
]

logger.info("ğŸš€ é–‹å§‹ç™¼é€ %s å€‹çˆ¬èŸ²ä»»å‹™", len(categories))

tasks = []

# ç‚ºæ¯å€‹åˆ†é¡å‰µå»ºä¸€å€‹ä»»å‹™
for category, job_type in categories:

    task = crawl_cake_jobs.s(category=category, job_type=job_type)
    task.apply_async(queue="crawler-queue")

    tasks.append(task)
    logger.info("ğŸ“¤ å·²ç™¼é€ä»»å‹™: %s | %s | ID: %s", category, job_type, task.id)

logger.info("âœ… æ‰€æœ‰ä»»å‹™å·²ç™¼é€å®Œæˆï¼Œå…± %s å€‹ä»»å‹™", len(tasks))
